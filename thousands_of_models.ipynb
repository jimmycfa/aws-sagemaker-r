{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and deploy thousands of SageMaker models on the cheap\n",
    "A major reason to use the cloud for developing, training, and deploying machine learning models is the ability to scale both quickly and cheaply. Here, we'll demonstrate the ability to train and deploy literally thousands of XGBoost models using Amazon SageMaker. \n",
    "\n",
    "We're going to bring our own XGBoost script here, but you should be able to modify this to use either a 1P algorithm or another model of your choice.\n",
    "\n",
    "Note, we are actually generating arbitrary data in the training cluster itself. The SageMaker API is going to expect a real CSV file in S3, so let's just post a dummy set there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.csv\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.csv\n",
    "286050,1995,2052,3,2.5,1.05,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting validation.csv\n"
     ]
    }
   ],
   "source": [
    "%%writefile validation.csv\n",
    "286050,1995,2052,3,2.5,1.05,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU awscli boto3 sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing all data to s3://sagemaker-us-east-1-181880743555/1K/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sagemaker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "prefix = '1K'\n",
    "full_s3_path = 's3://{}/{}/'.format(bucket, prefix)\n",
    "print ('Writing all data to {}'.format(full_s3_path))\n",
    "os.system('''aws s3 cp train.csv {}'''.format(full_s3_path))\n",
    "os.system('''aws s3 cp validation.csv {}'''.format(full_s3_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Next, we will loop through each of those cities and put them into batches. Each batch can be some arbitrary number, why not pick 100? That means we'll grab 100 cities at a time, package up the feature generation and model training into a single script, and run that on a dedicated SageMaker cluster.\n",
    "\n",
    "So, each batch will have 100 cities. For simplicity, let's assume the amount of data we need to train a single model is quite small, less than 1 GB. That means we don't need to distribute the jobs, we can run them on single nodes.\n",
    "\n",
    "In addition, to make the logging easier, we should actually train these models linearly, that is, one after the next. For 2000 models, that means we'll have 20 training jobs, and each training job will actually produce 100 models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop Through Batches and Run SageMaker Jobs on Spot\n",
    "Now, we're going to step through our 20 batches. For each batch, we'll do the following steps:\n",
    "1. Package up a script that loops through 100 cities, generates data, and trains models\n",
    "2. Run that script on a spot training job using the XGBoost estimator\n",
    "3. Ensure that the XGBoost estimator is called with`wait=False` so that the jobs start roughly at the same time.\n",
    "\n",
    "The big picture here is that we are running 20 training jobs on SageMaker spot instances, and each job is looping through our 100 models, massaging the training data, training a model, and writing it to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%writefile` not found (But cell magic `%%writefile` exists, did you mean that instead?).\n"
     ]
    }
   ],
   "source": [
    "%%writefile xgboost.py\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sagemaker_containers import entry_point\n",
    "from sagemaker_xgboost_container.data_utils import get_dmatrix\n",
    "\n",
    "DATA_PREFIX = '1K'\n",
    "MULTI_MODEL_ARTIFACTS = 'multi_model_artifacts'\n",
    "MAX_YEAR = 2019\n",
    "\n",
    "def gen_price(house):\n",
    "    base_price = int(house['SQUARE_FEET'] * 150)\n",
    "    price = int(base_price + (10000 * house['NUM_BEDROOMS']) + \\\n",
    "                               (15000 * house['NUM_BATHROOMS']) + \\\n",
    "                               (15000 * house['LOT_ACRES']) + \\\n",
    "                               (15000 * house['GARAGE_SPACES']) - \\\n",
    "                               (5000 * (MAX_YEAR - house['YEAR_BUILT'])))\n",
    "    return price\n",
    "\n",
    "def gen_random_house():\n",
    "    house = {'SQUARE_FEET':   int(np.random.normal(3000, 750)),\n",
    "              'NUM_BEDROOMS':  np.random.randint(2, 7),\n",
    "              'NUM_BATHROOMS': np.random.randint(2, 7) / 2,\n",
    "              'LOT_ACRES':     round(np.random.normal(1.0, 0.25), 2),\n",
    "              'GARAGE_SPACES': np.random.randint(0, 4),\n",
    "              'YEAR_BUILT':    min(MAX_YEAR, int(np.random.normal(1995, 10)))}\n",
    "\n",
    "    price = gen_price(house)\n",
    "    \n",
    "    return [price, house['YEAR_BUILT'],   house['SQUARE_FEET'], \n",
    "                    house['NUM_BEDROOMS'], house['NUM_BATHROOMS'], \n",
    "                    house['LOT_ACRES'],    house['GARAGE_SPACES']]\n",
    "\n",
    "def gen_houses(num_houses):\n",
    "    house_list = []\n",
    "    for i in range(num_houses):\n",
    "        house_list.append(gen_random_house())\n",
    "    df = pd.DataFrame(house_list, \n",
    "                       columns=['PRICE',        'YEAR_BUILT',    'SQUARE_FEET',  'NUM_BEDROOMS',\n",
    "                                'NUM_BATHROOMS','LOT_ACRES',     'GARAGE_SPACES'])\n",
    "    return df\n",
    "\n",
    "def split_data(df):\n",
    "    splits = [0.6, 0.3, 0.1]\n",
    "    \n",
    "    # split data into train and test sets\n",
    "    seed      =   7\n",
    "    val_size  = splits[1]\n",
    "    test_size = splits[2]\n",
    "    \n",
    "    num_samples = df.shape[0]\n",
    "    X1 = df.values[:num_samples, 1:] # keep only the features, skip the target, all rows\n",
    "    Y1 = df.values[:num_samples, :1] # keep only the target, all rows\n",
    "\n",
    "    # Use split ratios to divide up into train/val/test\n",
    "    X_train, X_val, y_train, y_val = \\\n",
    "        train_test_split(X1, Y1, test_size=(test_size + val_size), random_state=seed)\n",
    "    \n",
    "    # Of the remaining non-training samples, give proper ratio to validation and to test\n",
    "    X_test, X_test, y_test, y_test = \\\n",
    "        train_test_split(X_val, y_val, test_size=(test_size / (test_size + val_size)), \n",
    "                         random_state=seed)\n",
    "    \n",
    "    # reassemble the datasets with target in first column and features after that\n",
    "    train = np.concatenate([y_train, X_train], axis=1)\n",
    "    val   = np.concatenate([y_val,   X_val],   axis=1)\n",
    "    test  = np.concatenate([y_test,  X_test],  axis=1)\n",
    "\n",
    "    return train, val, test\n",
    "\n",
    "def save_data_locally(location, train, val, test):\n",
    "    os.makedirs(f'data/{location}/train')\n",
    "    np.savetxt( f'data/{location}/train/{location}_train.csv', train, delimiter=',', fmt='%.2f')\n",
    "    \n",
    "    os.makedirs(f'data/{location}/val')\n",
    "    np.savetxt(f'data/{location}/val/{location}_val.csv', val, delimiter=',', fmt='%.2f')\n",
    "    \n",
    "    os.makedirs(f'data/{location}/test')\n",
    "    np.savetxt(f'data/{location}/test/{location}_test.csv', test, delimiter=',', fmt='%.2f')\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"Deserialize and return fitted model.\n",
    "\n",
    "    Note that this should have the same name as the serialized model in the _xgb_train method\n",
    "    \"\"\"\n",
    "    model_file = 'xgboost-model'\n",
    "    booster = pkl.load(open(os.path.join(model_dir, model_file), 'rb'))\n",
    "    return booster\n",
    "        \n",
    "def parse_args():\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "#       Hyperparameters are described here.\n",
    "    parser.add_argument('--max_depth', type=int,)\n",
    "    parser.add_argument('--eta', type=float)\n",
    "    parser.add_argument('--gamma', type=int)\n",
    "    parser.add_argument('--min_child_weight', type=int)\n",
    "    parser.add_argument('--subsample', type=float)\n",
    "    parser.add_argument('--objective', type=str)\n",
    "    parser.add_argument('--num_round', type=int)\n",
    "\n",
    "#     Sagemaker specific arguments. Defaults are set in the environment variables.\n",
    "    parser.add_argument('--output_data_dir', type=str, default=os.environ.get('SM_OUTPUT_DATA_DIR'))\n",
    "    parser.add_argument('--model_dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n",
    "    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n",
    "    parser.add_argument('--validation', type=str, default=os.environ.get('SM_CHANNEL_VALIDATION'))\n",
    "    parser.add_argument('--sm_hosts', type=str, default=os.environ.get('SM_HOSTS'))\n",
    "    parser.add_argument('--sm_current_host', type=str, default=os.environ.get('SM_CURRENT_HOST'))\n",
    "\n",
    "    # pass in an argument for the models we need to generate data for\n",
    "    parser.add_argument('--n_models_to_train', type=str)\n",
    "    parser.add_argument('--job_id', type=str)\n",
    "    parser.add_argument('--n_jobs_to_run', type=str)\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    return args\n",
    "\n",
    "def get_cities(args):\n",
    "    '''\n",
    "    Takes the argparser, computes the set of models to train for this job id. For example\n",
    "        if job_id is 1, then compute\n",
    "            CITY_1 ... CITY_99\n",
    "        if job_id is 2, then compute\n",
    "            CITY_100 ... CITY_199\n",
    "    '''\n",
    "    rt = []\n",
    "    \n",
    "    job_id = int(args.job_id)\n",
    "        \n",
    "    total_models = int(args.n_models_to_train)\n",
    "    \n",
    "    models_per_job = round(total_models / int(args.n_jobs_to_run))\n",
    "\n",
    "    # compute the upper and lower bounds\n",
    "    upper_bound = job_id * models_per_job\n",
    "    lower_bound = upper_bound - models_per_job\n",
    "        \n",
    "    for model_i in range(lower_bound, upper_bound):\n",
    "        model_name = 'CITY_{}'.format(model_i)\n",
    "        rt.append(model_name)\n",
    "        \n",
    "    return rt\n",
    "\n",
    "\n",
    "def xgb_train(args, train, city):\n",
    "    \"\"\"Run xgb train on arguments given with rabit initialized.\n",
    "\n",
    "    This is our rabit execution function.\n",
    "\n",
    "    :param args_dict: Argument dictionary used to run xgb.train().\n",
    "    :param is_master: True if current node is master host in distributed training,\n",
    "                        or is running single node training job.\n",
    "                        Note that rabit_run will include this argument.\n",
    "    \"\"\"\n",
    "    train_hp = {\n",
    "        'max_depth': args.max_depth,\n",
    "        'eta': args.eta,\n",
    "        'gamma': args.gamma,\n",
    "        'min_child_weight': args.min_child_weight,\n",
    "        'subsample': args.subsample,\n",
    "        'objective': args.objective\n",
    "        }\n",
    "    \n",
    "    dtrain = get_dmatrix('data/{}/train/train.csv'.format(city), 'text/csv')\n",
    "    dval = get_dmatrix(args.validation, 'libsvm')\n",
    "    watchlist = [(dtrain, 'train'), (dval, 'validation')] if dval is not None else [(dtrain, 'train')]\n",
    "    \n",
    "    # why it doesn't see xgb.train here ... :( \n",
    "    booster = xgb.train(params=train_hp,\n",
    "                        dtrain=dtrain,\n",
    "                        evals=watchlist)\n",
    "\n",
    "    model_dir = args.model_dir\n",
    "    \n",
    "    model_location = model_dir + '/{}-model'.format(city)\n",
    "    pkl.dump(booster, open(model_location, 'wb'))\n",
    "    logging.info(\"Stored trained model at {}\".format(model_location))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    args = parse_args()\n",
    "    \n",
    "    os.mkdir('data')\n",
    "    \n",
    "    # loop through all of the models we need to train on this job\n",
    "    for city in get_cities(args):\n",
    "        \n",
    "        print ('about to generate data and train models for {}'.format(city))\n",
    "        \n",
    "        # generate the data we need\n",
    "        houses = gen_houses(num_houses = 1000)\n",
    "        train, val, test = split_data(houses)\n",
    "        \n",
    "        save_data_locally(city, train, val, test)\n",
    "        \n",
    "        print ('made it to saving data locally!!!')\n",
    "        \n",
    "        # train the model and store in model directory\n",
    "        xgb_train(args, train, city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.session import s3_input, Session\n",
    "from sagemaker.xgboost.estimator import XGBoost\n",
    "\n",
    "def get_estimator(full_s3_path, job_id, n_jobs_to_run, n_models_to_train):\n",
    "\n",
    "    hyperparams = {\n",
    "        \"max_depth\":\"5\",\n",
    "        \"eta\":\"0.2\",\n",
    "        \"gamma\":\"4\",\n",
    "        \"min_child_weight\":\"6\",\n",
    "        \"subsample\":\"0.7\",\n",
    "        \"verbose\":\"1\",\n",
    "        \"objective\":\"reg:linear\",\n",
    "        \"num_round\":\"50\",\n",
    "        \"job_id\": \"{}\".format(job_id),\n",
    "        \"n_models_to_train\": \"{}\".format(n_models_to_train),\n",
    "        \"n_jobs_to_run\": \"{}\".format(n_jobs_to_run)}\n",
    "\n",
    "    # need to add spot instances here\n",
    "    est = XGBoost(entry_point='xgboost.py',\n",
    "                    framework_version='1.0-1', # Note: framework_version is mandatory\n",
    "                    hyperparameters=hyperparams,\n",
    "                    role=sagemaker.get_execution_role(),\n",
    "                    train_instance_count=1, \n",
    "                    train_instance_type='ml.c5.xlarge',\n",
    "                    output_path=full_s3_path,\n",
    "                    train_use_spot_instances=1,\n",
    "                    train_max_run=(60 * 60 * 6),\n",
    "                    train_max_wait= (60 * 60 * 12))\n",
    "    \n",
    "    return est\n",
    "\n",
    "def run_job(est, full_s3_path, wait):\n",
    "    train_input = s3_input(full_s3_path, content_type='text/csv')\n",
    "    validation_input = s3_input(full_s3_path.format(bucket, prefix), content_type='text/csv')\n",
    "\n",
    "    est.fit({'train': train_input, 'validation': validation_input}, wait=wait)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-12 02:01:37 Starting - Starting the training job...\n",
      "2020-07-12 02:01:39 Starting - Launching requested ML instances......\n",
      "2020-07-12 02:02:56 Starting - Preparing the instances for training......\n",
      "2020-07-12 02:03:58 Downloading - Downloading input data\n",
      "2020-07-12 02:03:58 Training - Downloading the training image..\u001b[34mINFO:sagemaker-containers:Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34mINFO:sagemaker_xgboost_container.training:Invoking user training script.\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Module xgboost does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Generating setup.cfg\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python -m pip install . \u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: xgboost\n",
      "  Building wheel for xgboost (setup.py): started\n",
      "  Building wheel for xgboost (setup.py): finished with status 'done'\n",
      "  Created wheel for xgboost: filename=xgboost-1.0.0-py2.py3-none-any.whl size=9267 sha256=0fc1c7709e7f249c03ba76e7fcb13e05795fcf82c85ed5a42a355b6d49c8217a\n",
      "  Stored in directory: /home/model-server/tmp/pip-ephem-wheel-cache-4d9mw5tg/wheels/95/c1/85/65aaf48b35aba88c6e896d2fd04a4b69f1cee0d81ea32993ca\u001b[0m\n",
      "\u001b[34mSuccessfully built xgboost\u001b[0m\n",
      "\u001b[34mInstalling collected packages: xgboost\n",
      "  Attempting uninstall: xgboost\n",
      "    Found existing installation: xgboost 1.0.0\n",
      "    Uninstalling xgboost-1.0.0:\n",
      "      Successfully uninstalled xgboost-1.0.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed xgboost-1.0.0\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"validation\": \"/opt/ml/input/data/validation\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_xgboost_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"n_models_to_train\": \"2000\",\n",
      "        \"max_depth\": \"5\",\n",
      "        \"objective\": \"reg:linear\",\n",
      "        \"verbose\": \"1\",\n",
      "        \"eta\": \"0.2\",\n",
      "        \"job_id\": \"1\",\n",
      "        \"n_jobs_to_run\": \"20\",\n",
      "        \"num_round\": \"50\",\n",
      "        \"subsample\": \"0.7\",\n",
      "        \"gamma\": \"4\",\n",
      "        \"min_child_weight\": \"6\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"validation\": {\n",
      "            \"ContentType\": \"text/csv\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"ContentType\": \"text/csv\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"sagemaker-xgboost-2020-07-12-02-01-36-910\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-181880743555/sagemaker-xgboost-2020-07-12-02-01-36-910/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"xgboost\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"xgboost.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"eta\":\"0.2\",\"gamma\":\"4\",\"job_id\":\"1\",\"max_depth\":\"5\",\"min_child_weight\":\"6\",\"n_jobs_to_run\":\"20\",\"n_models_to_train\":\"2000\",\"num_round\":\"50\",\"objective\":\"reg:linear\",\"subsample\":\"0.7\",\"verbose\":\"1\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=xgboost.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"ContentType\":\"text/csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"ContentType\":\"text/csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\",\"validation\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=xgboost\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_xgboost_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-181880743555/sagemaker-xgboost-2020-07-12-02-01-36-910/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\",\"validation\":\"/opt/ml/input/data/validation\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_xgboost_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"eta\":\"0.2\",\"gamma\":\"4\",\"job_id\":\"1\",\"max_depth\":\"5\",\"min_child_weight\":\"6\",\"n_jobs_to_run\":\"20\",\"n_models_to_train\":\"2000\",\"num_round\":\"50\",\"objective\":\"reg:linear\",\"subsample\":\"0.7\",\"verbose\":\"1\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"ContentType\":\"text/csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"ContentType\":\"text/csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-xgboost-2020-07-12-02-01-36-910\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-181880743555/sagemaker-xgboost-2020-07-12-02-01-36-910/source/sourcedir.tar.gz\",\"module_name\":\"xgboost\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"xgboost.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--eta\",\"0.2\",\"--gamma\",\"4\",\"--job_id\",\"1\",\"--max_depth\",\"5\",\"--min_child_weight\",\"6\",\"--n_jobs_to_run\",\"20\",\"--n_models_to_train\",\"2000\",\"--num_round\",\"50\",\"--objective\",\"reg:linear\",\"--subsample\",\"0.7\",\"--verbose\",\"1\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_N_MODELS_TO_TRAIN=2000\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_DEPTH=5\u001b[0m\n",
      "\u001b[34mSM_HP_OBJECTIVE=reg:linear\u001b[0m\n",
      "\u001b[34mSM_HP_VERBOSE=1\u001b[0m\n",
      "\u001b[34mSM_HP_ETA=0.2\u001b[0m\n",
      "\u001b[34mSM_HP_JOB_ID=1\u001b[0m\n",
      "\u001b[34mSM_HP_N_JOBS_TO_RUN=20\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_ROUND=50\u001b[0m\n",
      "\u001b[34mSM_HP_SUBSAMPLE=0.7\u001b[0m\n",
      "\u001b[34mSM_HP_GAMMA=4\u001b[0m\n",
      "\u001b[34mSM_HP_MIN_CHILD_WEIGHT=6\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/miniconda3/bin:/:/miniconda3/lib/python3.6/site-packages/xgboost/dmlc-core/tracker:/miniconda3/lib/python36.zip:/miniconda3/lib/python3.6:/miniconda3/lib/python3.6/lib-dynload:/miniconda3/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python -m xgboost --eta 0.2 --gamma 4 --job_id 1 --max_depth 5 --min_child_weight 6 --n_jobs_to_run 20 --n_models_to_train 2000 --num_round 50 --objective reg:linear --subsample 0.7 --verbose 1\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mabout to generate data and train models for CITY_0\u001b[0m\n",
      "\u001b[34mmade it to saving data locally!!!\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/miniconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/miniconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/ml/code/xgboost.py\", line 206, in <module>\n",
      "    xgb_train(args, train, city)\n",
      "  File \"/opt/ml/code/xgboost.py\", line 178, in xgb_train\n",
      "    booster = xgb.train(params=train_hp,\u001b[0m\n",
      "\u001b[34mAttributeError: module 'xgboost' has no attribute 'train'\u001b[0m\n",
      "\u001b[34mERROR:sagemaker-containers:ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[34mCommand \"/miniconda3/bin/python -m xgboost --eta 0.2 --gamma 4 --job_id 1 --max_depth 5 --min_child_weight 6 --n_jobs_to_run 20 --n_models_to_train 2000 --num_round 50 --objective reg:linear --subsample 0.7 --verbose 1\"\u001b[0m\n",
      "\n",
      "2020-07-12 02:04:25 Uploading - Uploading generated training model\n",
      "2020-07-12 02:04:25 Failed - Training job failed\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job sagemaker-xgboost-2020-07-12-02-01-36-910: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"/miniconda3/bin/python -m xgboost --eta 0.2 --gamma 4 --job_id 1 --max_depth 5 --min_child_weight 6 --n_jobs_to_run 20 --n_models_to_train 2000 --num_round 50 --objective reg:linear --subsample 0.7 --verbose 1\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-c9965bbb6378>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_s3_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs_to_run\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_models_to_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mrun_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_s3_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-105-337753b890c1>\u001b[0m in \u001b[0;36mrun_job\u001b[0;34m(est, full_s3_path, wait)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mvalidation_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms3_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_s3_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'text/csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'validation'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalidation_input\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compilation_job_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0;31m# If logs are requested, call logs_for_jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"None\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type)\u001b[0m\n\u001b[1;32m   3075\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3076\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3077\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TrainingJobStatus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3078\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3079\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   2669\u001b[0m                 ),\n\u001b[1;32m   2670\u001b[0m                 \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Stopped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2671\u001b[0;31m                 \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2672\u001b[0m             )\n\u001b[1;32m   2673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job sagemaker-xgboost-2020-07-12-02-01-36-910: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"/miniconda3/bin/python -m xgboost --eta 0.2 --gamma 4 --job_id 1 --max_depth 5 --min_child_weight 6 --n_jobs_to_run 20 --n_models_to_train 2000 --num_round 50 --objective reg:linear --subsample 0.7 --verbose 1\""
     ]
    }
   ],
   "source": [
    "n_jobs_to_run = 20\n",
    "n_models_to_train = 2000\n",
    "\n",
    "for job_id in range(1, (n_jobs_to_run+1)):\n",
    "        \n",
    "    est = get_estimator(full_s3_path, job_id, n_jobs_to_run, n_models_to_train)\n",
    "    \n",
    "    run_job(est, full_s3_path, wait=False)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def launch_training_job(location, bucket):\n",
    "    # clear out old versions of the data\n",
    "    \n",
    "#     s3 = boto3.resource('s3')\n",
    "\n",
    "#     s3_bucket = s3.Bucket(bucket)\n",
    "    \n",
    "    full_input_prefix = f'{DATA_PREFIX}/model_prep/{location}'\n",
    "    \n",
    "    s3_bucket.objects.filter(Prefix=full_input_prefix + '/').delete()\n",
    "\n",
    "    # upload the entire set of data for all three channels\n",
    "#     local_folder = f'data/{location}'\n",
    "#     inputs = sagemaker_session.upload_data(path=local_folder, key_prefix=full_input_prefix)\n",
    "#     print(f'Training data uploaded: {inputs}')\n",
    "    \n",
    "#     _job = 'xgb-{}'.format(location.replace('_', '-'))\n",
    "    full_output_prefix = f'{DATA_PREFIX}/model_artifacts/{location}'\n",
    "    s3_output_path = f's3://{BUCKET}/{full_output_prefix}'\n",
    "\n",
    "    \n",
    "#     xgb = sagemaker.estimator.Estimator(XGBOOST_IMAGE, role, \n",
    "#                                         train_instance_count=1, train_instance_type=TRAIN_INSTANCE_TYPE,\n",
    "#                                         output_path=s3_output_path, base_job_name=_job,\n",
    "#                                         sagemaker_session=sagemaker_session)\n",
    "    \n",
    "#     xgb.set_hyperparameters(max_depth=5, eta=0.2, gamma=4, min_child_weight=6, subsample=0.8, silent=0, \n",
    "#                             early_stopping_rounds=5, objective='reg:linear', num_round=25) \n",
    "    \n",
    "#     DISTRIBUTION_MODE = 'FullyReplicated'\n",
    "    \n",
    "#     train_input = sagemaker.s3_input(s3_data=inputs+'/train', \n",
    "#                                      distribution=DISTRIBUTION_MODE, content_type='csv')\n",
    "    \n",
    "#     val_input   = sagemaker.s3_input(s3_data=inputs+'/val', \n",
    "#                                      distribution=DISTRIBUTION_MODE, content_type='csv')\n",
    "    \n",
    "#     remote_inputs = {'train': train_input, 'validation': val_input}\n",
    "\n",
    "#     xgb.fit(remote_inputs, wait=False)\n",
    "    \n",
    "    # Return the estimator object\n",
    "    return xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./thousands_of_models.ipynb to s3://my-bucket-for-fridays/notebooks/xgboost-by-the-thousands/thousands_of_models.ipynb\n",
      "upload: ./xgboost.py to s3://my-bucket-for-fridays/notebooks/xgboost-by-the-thousands/xgboost.py\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp \"thousands_of_models.ipynb\" s3://my-bucket-for-fridays/notebooks/xgboost-by-the-thousands/\n",
    "!aws s3 cp \"xgboost.py\" s3://my-bucket-for-fridays/notebooks/xgboost-by-the-thousands/"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
