{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and deploy thousands of SageMaker models on the cheap\n",
    "A major reason to use the cloud for developing, training, and deploying machine learning models is the ability to scale both quickly and cheaply. Here, we'll demonstrate the ability to train and deploy literally thousands of XGBoost models using Amazon SageMaker. \n",
    "\n",
    "We're going to bring our own XGBoost script here, but you should be able to modify this to use either a 1P algorithm or another model of your choice.\n",
    "\n",
    "Note, we are actually generating arbitrary data in the training cluster itself. The SageMaker API is going to expect a real CSV file in S3, so let's just post a dummy set there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.csv\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.csv\n",
    "286050,1995,2052,3,2.5,1.05,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting validation.csv\n"
     ]
    }
   ],
   "source": [
    "%%writefile validation.csv\n",
    "286050,1995,2052,3,2.5,1.05,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "  Using cached pip-20.2.2-py2.py3-none-any.whl (1.5 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 20.0.2\n",
      "    Uninstalling pip-20.0.2:\n",
      "      Successfully uninstalled pip-20.0.2\n",
      "Successfully installed pip-20.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU awscli boto3 \n",
    "#!pip install -qU sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: sagemaker==1.72.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (1.72.0)\n",
      "Requirement already satisfied, skipping upgrade: protobuf3-to-dict>=0.1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker==1.72.0) (0.1.5)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.19.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker==1.72.0) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: smdebug-rulesconfig==0.1.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker==1.72.0) (0.1.4)\n",
      "Requirement already satisfied, skipping upgrade: boto3>=1.14.12 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker==1.72.0) (1.14.48)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata>=1.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker==1.72.0) (1.5.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.9.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker==1.72.0) (1.18.1)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker==1.72.0) (3.12.4)\n",
      "Requirement already satisfied, skipping upgrade: packaging>=20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker==1.72.0) (20.3)\n",
      "Requirement already satisfied, skipping upgrade: enum34>=1.1.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker==1.72.0) (1.1.10)\n",
      "Requirement already satisfied, skipping upgrade: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from protobuf3-to-dict>=0.1.5->sagemaker==1.72.0) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker==1.72.0) (0.9.4)\n",
      "Requirement already satisfied, skipping upgrade: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker==1.72.0) (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: botocore<1.18.0,>=1.17.48 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker==1.72.0) (1.17.48)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker==1.72.0) (2.2.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from protobuf>=3.1->sagemaker==1.72.0) (46.1.3.post20200330)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from packaging>=20.0->sagemaker==1.72.0) (2.4.6)\n",
      "Requirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.18.0,>=1.17.48->boto3>=1.14.12->sagemaker==1.72.0) (0.15.2)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.26,>=1.20; python_version != \"3.4\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.18.0,>=1.17.48->boto3>=1.14.12->sagemaker==1.72.0) (1.25.8)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.18.0,>=1.17.48->boto3>=1.14.12->sagemaker==1.72.0) (2.8.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install sagemaker==1.72.0 -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-1.2.0-py3-none-manylinux2010_x86_64.whl (148.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 148.9 MB 28 kB/s s eta 0:00:01    |█████▌                          | 25.6 MB 24.6 MB/s eta 0:00:06�████████████▊| 147.8 MB 86.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from xgboost) (1.18.1)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from xgboost) (1.4.1)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing all data to s3://sagemaker-us-east-1-572539092864/1K/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sagemaker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "prefix = '1K'\n",
    "full_s3_path = 's3://{}/{}/'.format(bucket, prefix)\n",
    "print ('Writing all data to {}'.format(full_s3_path))\n",
    "os.system('''aws s3 cp train.csv {}'''.format(full_s3_path))\n",
    "os.system('''aws s3 cp validation.csv {}'''.format(full_s3_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Next, we will loop through each of those cities and put them into batches. Each batch can be some arbitrary number, why not pick 100? That means we'll grab 100 cities at a time, package up the feature generation and model training into a single script, and run that on a dedicated SageMaker cluster.\n",
    "\n",
    "So, each batch will have 100 cities. For simplicity, let's assume the amount of data we need to train a single model is quite small, less than 1 GB. That means we don't need to distribute the jobs, we can run them on single nodes.\n",
    "\n",
    "In addition, to make the logging easier, we should actually train these models linearly, that is, one after the next. For 2000 models, that means we'll have 20 training jobs, and each training job will actually produce 100 models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop Through Batches and Run SageMaker Jobs on Spot\n",
    "Now, we're going to step through our 20 batches. For each batch, we'll do the following steps:\n",
    "1. Package up a script that loops through 100 cities, generates data, and trains models\n",
    "2. Run that script on a spot training job using the XGBoost estimator\n",
    "3. Ensure that the XGBoost estimator is called with`wait=False` so that the jobs start roughly at the same time.\n",
    "\n",
    "The big picture here is that we are running 20 training jobs on SageMaker spot instances, and each job is looping through our 100 models, massaging the training data, training a model, and writing it to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing xgboost_script.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile xgboost_script.py\n",
    "\n",
    "import argparse\n",
    "import glob\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sagemaker_containers import entry_point\n",
    "from sagemaker_xgboost_container.data_utils import get_dmatrix\n",
    "\n",
    "DATA_PREFIX = '1K'\n",
    "MULTI_MODEL_ARTIFACTS = 'multi_model_artifacts'\n",
    "MAX_YEAR = 2019\n",
    "\n",
    "# data types used by data_utils\n",
    "CSV = 'csv'\n",
    "LIBSVM = 'libsvm'\n",
    "\n",
    "def gen_price(house):\n",
    "    base_price = int(house['SQUARE_FEET'] * 150)\n",
    "    price = int(base_price + (10000 * house['NUM_BEDROOMS']) + \\\n",
    "                               (15000 * house['NUM_BATHROOMS']) + \\\n",
    "                               (15000 * house['LOT_ACRES']) + \\\n",
    "                               (15000 * house['GARAGE_SPACES']) - \\\n",
    "                               (5000 * (MAX_YEAR - house['YEAR_BUILT'])))\n",
    "    return price\n",
    "\n",
    "def gen_random_house():\n",
    "    house = {'SQUARE_FEET':   int(np.random.normal(3000, 750)),\n",
    "              'NUM_BEDROOMS':  np.random.randint(2, 7),\n",
    "              'NUM_BATHROOMS': np.random.randint(2, 7) / 2,\n",
    "              'LOT_ACRES':     round(np.random.normal(1.0, 0.25), 2),\n",
    "              'GARAGE_SPACES': np.random.randint(0, 4),\n",
    "              'YEAR_BUILT':    min(MAX_YEAR, int(np.random.normal(1995, 10)))}\n",
    "\n",
    "    price = gen_price(house)\n",
    "    \n",
    "    return [price, house['YEAR_BUILT'],   house['SQUARE_FEET'], \n",
    "                    house['NUM_BEDROOMS'], house['NUM_BATHROOMS'], \n",
    "                    house['LOT_ACRES'],    house['GARAGE_SPACES']]\n",
    "\n",
    "def gen_houses(num_houses):\n",
    "    house_list = []\n",
    "    for i in range(num_houses):\n",
    "        house_list.append(gen_random_house())\n",
    "    df = pd.DataFrame(house_list, \n",
    "                       columns=['PRICE',        'YEAR_BUILT',    'SQUARE_FEET',  'NUM_BEDROOMS',\n",
    "                                'NUM_BATHROOMS','LOT_ACRES',     'GARAGE_SPACES'])\n",
    "    return df\n",
    "\n",
    "def split_data(df):\n",
    "    splits = [0.6, 0.3, 0.1]\n",
    "    \n",
    "    # split data into train and test sets\n",
    "    seed      =   7\n",
    "    val_size  = splits[1]\n",
    "    test_size = splits[2]\n",
    "    \n",
    "    num_samples = df.shape[0]\n",
    "    X1 = df.values[:num_samples, 1:] # keep only the features, skip the target, all rows\n",
    "    Y1 = df.values[:num_samples, :1] # keep only the target, all rows\n",
    "\n",
    "    # Use split ratios to divide up into train/val/test\n",
    "    X_train, X_val, y_train, y_val = \\\n",
    "        train_test_split(X1, Y1, test_size=(test_size + val_size), random_state=seed)\n",
    "    \n",
    "    # Of the remaining non-training samples, give proper ratio to validation and to test\n",
    "    X_test, X_test, y_test, y_test = \\\n",
    "        train_test_split(X_val, y_val, test_size=(test_size / (test_size + val_size)), \n",
    "                         random_state=seed)\n",
    "    \n",
    "    # reassemble the datasets with target in first column and features after that\n",
    "    train = np.concatenate([y_train, X_train], axis=1)\n",
    "    val   = np.concatenate([y_val,   X_val],   axis=1)\n",
    "    test  = np.concatenate([y_test,  X_test],  axis=1)\n",
    "\n",
    "    return train, val, test\n",
    "\n",
    "def save_data_locally(location, train, val, test):\n",
    "    os.makedirs(f'data/{location}/train')\n",
    "    np.savetxt( f'data/{location}/train/{location}_train.csv', train, delimiter=',', fmt='%.2f')\n",
    "    \n",
    "    os.makedirs(f'data/{location}/val')\n",
    "    np.savetxt(f'data/{location}/val/{location}_val.csv', val, delimiter=',', fmt='%.2f')\n",
    "    \n",
    "    os.makedirs(f'data/{location}/test')\n",
    "    np.savetxt(f'data/{location}/test/{location}_test.csv', test, delimiter=',', fmt='%.2f')\n",
    "    \n",
    "    for fname in glob.glob(f'data/{location}/train'):\n",
    "        print('Found train file: {}'.format(fname))\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"Deserialize and return fitted model.\n",
    "\n",
    "    Note that this should have the same name as the serialized model in the _xgb_train method\n",
    "    \"\"\"\n",
    "    model_file = 'xgboost-model'\n",
    "    booster = pkl.load(open(os.path.join(model_dir, model_file), 'rb'))\n",
    "    return booster\n",
    "        \n",
    "def parse_args():\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "#       Hyperparameters are described here.\n",
    "    parser.add_argument('--max_depth', type=int,)\n",
    "    parser.add_argument('--eta', type=float)\n",
    "    parser.add_argument('--gamma', type=int)\n",
    "    parser.add_argument('--min_child_weight', type=int)\n",
    "    parser.add_argument('--subsample', type=float)\n",
    "    parser.add_argument('--objective', type=str)\n",
    "    parser.add_argument('--num_round', type=int)\n",
    "\n",
    "#     Sagemaker specific arguments. Defaults are set in the environment variables.\n",
    "    parser.add_argument('--output_data_dir', type=str, default=os.environ.get('SM_OUTPUT_DATA_DIR'))\n",
    "    parser.add_argument('--model_dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n",
    "    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n",
    "    parser.add_argument('--validation', type=str, default=os.environ.get('SM_CHANNEL_VALIDATION'))\n",
    "    parser.add_argument('--sm_hosts', type=str, default=os.environ.get('SM_HOSTS'))\n",
    "    parser.add_argument('--sm_current_host', type=str, default=os.environ.get('SM_CURRENT_HOST'))\n",
    "\n",
    "    # pass in an argument for the models we need to generate data for\n",
    "    parser.add_argument('--n_models_to_train', type=str)\n",
    "    parser.add_argument('--job_id', type=str)\n",
    "    parser.add_argument('--n_jobs_to_run', type=str)\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    return args\n",
    "\n",
    "def get_cities(args):\n",
    "    '''\n",
    "    Takes the argparser, computes the set of models to train for this job id. For example\n",
    "        if job_id is 1, then compute\n",
    "            CITY_1 ... CITY_99\n",
    "        if job_id is 2, then compute\n",
    "            CITY_100 ... CITY_199\n",
    "    '''\n",
    "    rt = []\n",
    "    \n",
    "    job_id = int(args.job_id)\n",
    "        \n",
    "    total_models = int(args.n_models_to_train)\n",
    "    \n",
    "    models_per_job = round(total_models / int(args.n_jobs_to_run))\n",
    "\n",
    "    # compute the upper and lower bounds\n",
    "    upper_bound = job_id * models_per_job\n",
    "    lower_bound = upper_bound - models_per_job\n",
    "        \n",
    "    for model_i in range(lower_bound, upper_bound):\n",
    "        model_name = 'CITY_{}'.format(model_i)\n",
    "        rt.append(model_name)\n",
    "        \n",
    "    return rt\n",
    "\n",
    "\n",
    "def xgb_train(args, train, val, city):\n",
    "    \"\"\"Run xgb train on arguments given with rabit initialized.\n",
    "\n",
    "    This is our rabit execution function.\n",
    "\n",
    "    :param args_dict: Argument dictionary used to run xgb.train().\n",
    "    :param is_master: True if current node is master host in distributed training,\n",
    "                        or is running single node training job.\n",
    "                        Note that rabit_run will include this argument.\n",
    "    \"\"\"\n",
    "    train_hp = {\n",
    "        'max_depth': args.max_depth,\n",
    "        'eta': args.eta,\n",
    "        'gamma': args.gamma,\n",
    "        'min_child_weight': args.min_child_weight,\n",
    "        'subsample': args.subsample,\n",
    "        'objective': args.objective\n",
    "        }\n",
    "        \n",
    "    # point to directory containing train/test files in CSV format\n",
    "    dtrain = get_dmatrix(f'data/{city}/train/', CSV)\n",
    "    dval   = get_dmatrix(f'data/{city}/val/', CSV)\n",
    "    \n",
    "    watchlist = [(dtrain, 'train'), (dval, 'validation')] if dval is not None else [(dtrain, 'train')]\n",
    "    \n",
    "    # why it doesn't see xgb.train here ... :( \n",
    "    booster = xgb.train(params=train_hp,\n",
    "                        dtrain=dtrain,\n",
    "                        evals=watchlist)\n",
    "\n",
    "    model_dir = args.model_dir\n",
    "    \n",
    "    model_location = model_dir + '/{}-model'.format(city)\n",
    "    pkl.dump(booster, open(model_location, 'wb'))\n",
    "    logging.info(\"Stored trained model at {}\".format(model_location))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    args = parse_args()\n",
    "    \n",
    "    os.mkdir('data')\n",
    "    \n",
    "    # loop through all of the models we need to train on this job\n",
    "    for city in get_cities(args):\n",
    "        \n",
    "        print ('Starting to generate data and train models for {}'.format(city))\n",
    "        \n",
    "        # generate the data we need\n",
    "        houses = gen_houses(num_houses = 1000)\n",
    "        train, val, test = split_data(houses)\n",
    "        \n",
    "        print('Channel sizes: train={} val={} test={} '.format(len(train), len(val), len(test)))\n",
    "                \n",
    "        save_data_locally(city, train, val, test)\n",
    "        \n",
    "        print ('Completed saving data locally!!!')\n",
    "        \n",
    "        # train the model and store in model directory\n",
    "        xgb_train(args, train, val, city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.session import s3_input\n",
    "from sagemaker.xgboost.estimator import XGBoost\n",
    "\n",
    "def get_estimator(full_s3_path, job_id, n_jobs_to_run, n_models_to_train):\n",
    "\n",
    "    hyperparams = {\n",
    "        \"max_depth\":\"5\",\n",
    "        \"eta\":\"0.2\",\n",
    "        \"gamma\":\"4\",\n",
    "        \"min_child_weight\":\"6\",\n",
    "        \"subsample\":\"0.7\",\n",
    "        \"verbose\":\"1\",\n",
    "        \"objective\":\"reg:linear\",\n",
    "        \"num_round\":\"50\",\n",
    "        \"job_id\": \"{}\".format(job_id),\n",
    "        \"n_models_to_train\": \"{}\".format(n_models_to_train),\n",
    "        \"n_jobs_to_run\": \"{}\".format(n_jobs_to_run)}\n",
    "\n",
    "    # need to add spot instances here\n",
    "    est = XGBoost(entry_point='xgboost_script.py',\n",
    "                    framework_version='1.0-1', # Note: framework_version is mandatory\n",
    "                    hyperparameters=hyperparams,\n",
    "                    role=sagemaker.get_execution_role(),\n",
    "                    train_instance_count=1, \n",
    "                    train_instance_type='ml.c5.xlarge',\n",
    "                    output_path=full_s3_path,\n",
    "                    train_use_spot_instances=1,\n",
    "                    train_max_run=(60 * 60 * 6),\n",
    "                    train_max_wait= (60 * 60 * 12))\n",
    "    \n",
    "    return est\n",
    "\n",
    "def run_job(est, full_s3_path, wait):\n",
    "    train_input = sagemaker.s3_input(full_s3_path, content_type='text/csv')\n",
    "    validation_input = sagemaker.s3_input(full_s3_path.format(bucket, prefix), content_type='text/csv')\n",
    "\n",
    "    est.fit({'train': train_input, 'validation': validation_input}, wait=wait)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "n_jobs_to_run = 20\n",
    "n_models_to_train = 2000\n",
    "\n",
    "for job_id in range(1, (n_jobs_to_run+1)):\n",
    "        \n",
    "    est = get_estimator(full_s3_path, job_id, n_jobs_to_run, n_models_to_train)\n",
    "    \n",
    "    run_job(est, full_s3_path, wait=False)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def launch_training_job(location, bucket):\n",
    "    # clear out old versions of the data\n",
    "    \n",
    "#     s3 = boto3.resource('s3')\n",
    "\n",
    "#     s3_bucket = s3.Bucket(bucket)\n",
    "    \n",
    "    full_input_prefix = f'{DATA_PREFIX}/model_prep/{location}'\n",
    "    \n",
    "    s3_bucket.objects.filter(Prefix=full_input_prefix + '/').delete()\n",
    "\n",
    "    # upload the entire set of data for all three channels\n",
    "#     local_folder = f'data/{location}'\n",
    "#     inputs = sagemaker_session.upload_data(path=local_folder, key_prefix=full_input_prefix)\n",
    "#     print(f'Training data uploaded: {inputs}')\n",
    "    \n",
    "#     _job = 'xgb-{}'.format(location.replace('_', '-'))\n",
    "    full_output_prefix = f'{DATA_PREFIX}/model_artifacts/{location}'\n",
    "    s3_output_path = f's3://{BUCKET}/{full_output_prefix}'\n",
    "\n",
    "    \n",
    "#     xgb = sagemaker.estimator.Estimator(XGBOOST_IMAGE, role, \n",
    "#                                         train_instance_count=1, train_instance_type=TRAIN_INSTANCE_TYPE,\n",
    "#                                         output_path=s3_output_path, base_job_name=_job,\n",
    "#                                         sagemaker_session=sagemaker_session)\n",
    "    \n",
    "#     xgb.set_hyperparameters(max_depth=5, eta=0.2, gamma=4, min_child_weight=6, subsample=0.8, silent=0, \n",
    "#                             early_stopping_rounds=5, objective='reg:linear', num_round=25) \n",
    "    \n",
    "#     DISTRIBUTION_MODE = 'FullyReplicated'\n",
    "    \n",
    "#     train_input = sagemaker.s3_input(s3_data=inputs+'/train', \n",
    "#                                      distribution=DISTRIBUTION_MODE, content_type='csv')\n",
    "    \n",
    "#     val_input   = sagemaker.s3_input(s3_data=inputs+'/val', \n",
    "#                                      distribution=DISTRIBUTION_MODE, content_type='csv')\n",
    "    \n",
    "#     remote_inputs = {'train': train_input, 'validation': val_input}\n",
    "\n",
    "#     xgb.fit(remote_inputs, wait=False)\n",
    "    \n",
    "    # Return the estimator object\n",
    "    return xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## !aws s3 cp \"thousands_of_models.ipynb\" s3://my-bucket-for-fridays/notebooks/xgboost-by-the-thousands/\n",
    "## !aws s3 cp \"xgboost.py\" s3://my-bucket-for-fridays/notebooks/xgboost-by-the-thousands/"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
